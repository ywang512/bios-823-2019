{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Low Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [ ] Foundations\n",
    "    - [ ] Functional programming (Spark RDDs)\n",
    "    - [ ] Use of pandas (Spark DataFramej\n",
    "    - [ ] Use of SQL (Spark SQL)\n",
    "- [ ] Catalyst optimizer\n",
    "    - [ ] Analysis\n",
    "        - [ ] DataFrame\n",
    "        - [ ] SQL AST\n",
    "    - [ ] Logical plan\n",
    "        - [ ] Catalog revolution of names\n",
    "        - [ ] Rule-based optimization\n",
    "            - [ ] Boolean simplification\n",
    "            - [ ] Predicate pushdown\n",
    "            - [ ] Constant folding\n",
    "            - [ ] Projection pruning\n",
    "    - [ ] Physical plan\n",
    "        - [ ] Convert to RDD\n",
    "        - [ ] Cost-based optimization\n",
    "    - [ ] Code generation\n",
    "        - [ ] Project Tungsten\n",
    "        - [ ] Performs role of compiler\n",
    "        - [ ] Generates Java bytecode\n",
    "\n",
    "![img](https://mapr.com/blog/how-spark-runs-your-applications/assets/image9.png)                         \n",
    "                         \n",
    "Resilient distributed datasets (RDD)\n",
    "- [ ] In-memory distributed collections\n",
    "- [ ] Removes need to write to disk for fault tolerance\n",
    "- [ ] Resilience\n",
    "    - [ ] Reconstruct from lineage\n",
    "    - [ ] Immutable\n",
    "- [ ] Distributed\n",
    "    - [ ] RDD data live in one or more partitions\n",
    "- [ ] Dataset\n",
    "    - [ ] Consists of records\n",
    "    - [ ] Each partition consists of distinct set of records that can be operated on independently\n",
    "    - [ ] Shared nothing philosophy\n",
    "    \n",
    "![img](https://miro.medium.com/max/1152/1*l2MUHFvWfcdiUbh7Y-fM5Q.png)    \n",
    " \n",
    " \n",
    "- [ ] Loading data into RDDs\n",
    "    - [ ] Programmatically\n",
    "        - [ ] range()\n",
    "        - [ ] parallelize()\n",
    "    - [ ] From file\n",
    "        - [ ] Compression\n",
    "            - [ ] Splittable and non-splittable formats\n",
    "            - [ ] Non-splittable files cannot be distributed\n",
    "            - [ ] Splittable formats - LZO, Snappy\n",
    "            - [ ] Non-splittable formats - gzip, zip\n",
    "        - [ ] Data locality\n",
    "            - [ ] Worker partitions from nearby DFS partitions\n",
    "            - [ ] Default partition size is 128 MB\n",
    "            - [ ] Local file system\n",
    "            - [ ] Networked filesystem\n",
    "            - [ ] Distributed filesystem\n",
    "        - [ ] textFile()\n",
    "        - [ ] WoleTextFiles()\n",
    "    - [ ] From data resource\n",
    "    - [ ] From stream\n",
    "- [ ] Persistence\n",
    "    - [ ] persist)\n",
    "    - [ ] cache()\n",
    "- [ ] Types of RDDs\n",
    "    - [ ] Base RDD\n",
    "    - [ ] Pair RDD\n",
    "    - [ ] Double RDD\n",
    "    - [ ] Many others\n",
    "\n",
    "Base RDD\n",
    "\n",
    "- [ ] Narrow transformations\n",
    "    - [ ] map()\n",
    "    - [ ] filter()\n",
    "    - [ ] flatMap(j\n",
    "    - [ ] distinct()\n",
    "- [ ] Broad transformations\n",
    "    - [ ] reduce()\n",
    "    - [ ] groupby()\n",
    "    - [ ] sortBy()\n",
    "    - [ ] join()\n",
    "- [ ] Actions\n",
    "    - [ ] count()\n",
    "    - [ ] take()\n",
    "    - [ ] takeOrdered()\n",
    "    - [ ] top()\n",
    "    - [ ] collect()\n",
    "    - [ ] saveAsTextFile()\n",
    "    - [ ] first()\n",
    "    - [ ] reduce()\n",
    "    - [ ] fold()\n",
    "    - [ ] aggregate()\n",
    "    - [ ] foreach()\n",
    "    \n",
    "PairedRDD\n",
    "\n",
    "- [ ] Dictionary functions\n",
    "    - [ ] keys()\n",
    "    - [ ] values()\n",
    "    - [ ] keyBy()\n",
    "- [ ] Functional transformations\n",
    "    - [ ] mapValues()\n",
    "    - [ ] flatMapValues()\n",
    "- [ ] Grouping, sorting and aggregation\n",
    "    - [ ] groupByKey()\n",
    "    - [ ] reduceByKey()\n",
    "    - [ ] foldByKey()\n",
    "    - [ ] sortByKey()\n",
    "- [ ] Joins\n",
    "    - [ ] Join large by small\n",
    "    - [ ] join()\n",
    "    - [ ] leftOuterJoin()\n",
    "    - [ ] rightOuterJoin()\n",
    "    - [ ] fullOuterJoin()\n",
    "    - [ ] cogroup()\n",
    "    - [ ] cartesian()\n",
    "- [ ] Set operations\n",
    "    - [ ] union()\n",
    "    - [ ] intersection(j\n",
    "    - [ ] subtract()\n",
    "    - [ ] subtractByKey()\n",
    "\n",
    "Numeric RDD\n",
    "\n",
    "- [ ] min()\n",
    "- [ ] max()\n",
    "- [ ] sum()\n",
    "- [ ] mean()\n",
    "- [ ] stdev()\n",
    "- [ ] variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of a Spark Application\n",
    "\n",
    "### Big picture\n",
    "\n",
    "You will type your commands iin a local Spark session, and the SparkContext will take care of running your instructions distributed across the workers (executors) on a cluster. Each executor can have 1 or more CPU cores, its own memory cahe, and is responsible for handling its own distributed tasks. Communicaiton between local and workers and between worker and worker is handled by a cluster manager. \n",
    "\n",
    "![Spark components](http://spark.apache.org/docs/latest/img/cluster-overview.png)\n",
    "\n",
    "Source: http://spark.apache.org/docs/latest/img/cluster-overview.png\n",
    "\n",
    "### Organizaiton of Spark tasks\n",
    "\n",
    "Spark organizes tasks that can be performed without exchanging data across partitions into stages. The sequecne of tasks to be perfomed are laid out as a Directed Acyclic Graph (DAG). Tasks are differenitated into transforms (lazy evalutation - just add to DAG) and actions (eager evaluation - execute the specified path in the DAG). Note that calculations are not cached unless requested. Hence if you have triggered the action after RDD3 in the figure, then trigger the aciton after RDD6, RDD2 will be re-generated from RDD1 twice. We can avoid the re-calculation by persisting or cacheing RDD2.\n",
    "\n",
    "![Spark stages](https://image.slidesharecdn.com/mapreducevsspark-150512052504-lva1-app6891/95/map-reduce-vs-spark-16-638.jpg?cb=1431408380)\n",
    "\n",
    "Source: https://image.slidesharecdn.com/mapreducevsspark-150512052504-lva1-app6891/95/map-reduce-vs-spark-16-638.jpg?cb=1431408380"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext\n",
    "\n",
    "A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. Here we set it up to use local nodes - the argument `locals[*]` means to use the local machine as the cluster, using as many worker threads as there are cores. You can also explicitly set the number of cores with `locals[k]` where `k` is an integer.\n",
    "\n",
    "With Saprk 2.0 onwards, there is also a SparkSession that manages DataFrames, which is the preferred abstraction for working in Spark. However DataFrames are composed of RDDs, and it is still necesaary to understand how to use and mainpulate RDDs for low level operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your setup, you many have to import SparkContext. This is not necessary in our Docker containers as we will be using `livy`.\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local[*]')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1497</td><td>application_1562678715323_0061</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://rapid-778.oit.duke.edu:8088/proxy/application_1562678715323_0061/\">Link</a></td><td><a target=\"_blank\" href=\"http://rapid-782.oit.duke.edu:8042/node/containerlogs/container_e25_1562678715323_0061_01_000001/user12029\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'2.2.0.2.6.4.0-91'"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in an RDD is distributed across partitions. It is most efficient if data does not have to be transferred across partitions. We can see the default minimumn number of partitions, and the actual number in an RDD later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an RDD\n",
    "\n",
    "The RDD (Resilient Distributed Dataset) is a data storage abstraction - you can work with it as though it were single unit, while it may actually be distributed over many nodes in the computing cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A first example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribute the data set to the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:489"
     ]
    }
   ],
   "source": [
    "xs = sc.parallelize(range(10))\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "xs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return all data within each partition as a list. Note that the glom() operation operates on the distributed workers without centralizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [2, 3], [4, 5], [6, 7, 8, 9]]"
     ]
    }
   ],
   "source": [
    "xs.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep even numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[2] at RDD at PythonRDD.scala:48"
     ]
    }
   ],
   "source": [
    "xs = xs.filter(lambda x: x % 2 == 0)\n",
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Square all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[3] at RDD at PythonRDD.scala:48"
     ]
    }
   ],
   "source": [
    "xs = xs.map(lambda x: x**2)\n",
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code and return the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 16, 36, 64]"
     ]
    }
   ],
   "source": [
    "xs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce also triggers a calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120"
     ]
    }
   ],
   "source": [
    "xs.reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A common Spark idiom chains mutiple functions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 16, 36, 64]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sc.parallelize(range(10))\n",
    "    .filter(lambda x: x % 2 == 0)\n",
    "    .map(lambda x: x**2)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions and transforms\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **transform** maps an RDD to another RDD - it is a lazy operation. To actually perform any work, we need to apply an **action**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize(np.random.randint(1, 6, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 2, 3, 4, 5, 3, 2, 5, 1]"
     ]
    }
   ],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 2, 3, 4]"
     ]
    }
   ],
   "source": [
    "x.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    }
   ],
   "source": [
    "x.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 5, 4, 3]"
     ]
    }
   ],
   "source": [
    "x.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 5, 5, 4, 4, 1, 1, 1, 2, 1, 5, 2, 3, 4]"
     ]
    }
   ],
   "source": [
    "x.takeSample(True, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10"
     ]
    }
   ],
   "source": [
    "x.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 5, 2, 3]"
     ]
    }
   ],
   "source": [
    "x.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {1: 1, 2: 2, 3: 3, 4: 1, 5: 3})"
     ]
    }
   ],
   "source": [
    "x.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33"
     ]
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5"
     ]
    }
   ],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3"
     ]
    }
   ],
   "source": [
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 10, mean: 3.3, stdev: 1.3453624047073711, max: 5.0, min: 1.0)"
     ]
    }
   ],
   "source": [
    "x.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce, fold and aggregate actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the API**:\n",
    "\n",
    "- reduce(f)\n",
    "\n",
    "> Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally.\n",
    "\n",
    "- fold(zeroValue, op)\n",
    "\n",
    "> Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral “zero value.”\n",
    "\n",
    "> The function op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.\n",
    "\n",
    "> This behaves somewhat differently from fold operations implemented for non-distributed collections in functional languages like Scala. This fold operation may be applied to partitions individually, and then fold those results into the final result, rather than apply the fold to each element sequentially in some defined ordering. For functions that are not commutative, the result may differ from that of a fold applied to a non-distributed collection.\n",
    "\n",
    "- aggregate(zeroValue, seqOp, combOp)\n",
    "\n",
    "> Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”\n",
    "\n",
    "> The functions op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.\n",
    "\n",
    "> The first function (seqOp) can return a different result type, U, than the type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U\n",
    "\n",
    "Notes:\n",
    "\n",
    "- All 3 operations take a binary op with signature op(accumulator, operand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize(np.random.randint(1, 10, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 5, 9, 5, 9, 9, 3, 5, 7, 4, 4, 3]"
     ]
    }
   ],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max** using reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    }
   ],
   "source": [
    "x.reduce(lambda x, y: x if x > y else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sum** using `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71"
     ]
    }
   ],
   "source": [
    "x.reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sum** using fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71"
     ]
    }
   ],
   "source": [
    "x.fold(0, lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prod** using reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734832000"
     ]
    }
   ],
   "source": [
    "x.reduce(lambda x, y: x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prod** using fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734832000"
     ]
    }
   ],
   "source": [
    "x.fold(1, lambda x, y: x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sum** using aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71"
     ]
    }
   ],
   "source": [
    "x.aggregate(0, lambda x, y: x + y, lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count** using aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12"
     ]
    }
   ],
   "source": [
    "x.aggregate(0, lambda acc, _: acc + 1, lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mean** using aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5"
     ]
    }
   ],
   "source": [
    "sum_count = x.aggregate([0,0], \n",
    "                        lambda acc, x: (acc[0]+x, acc[1]+1), \n",
    "                        lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1]+ acc2[1]))\n",
    "sum_count[0]/sum_count[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Be very careful wiht fold and aggregate - the zero value must be \"neutral\".  The behhavior can be different from Python's reduce with an initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([8, 5, 9, 5, 9, 9, 3, 5, 7, 4, 4, 3])"
     ]
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71"
     ]
    }
   ],
   "source": [
    "sum(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explain the results shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72"
     ]
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y, xs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76"
     ]
    }
   ],
   "source": [
    "x.fold(1, lambda acc, val: acc + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76"
     ]
    }
   ],
   "source": [
    "x.aggregate(1, lambda x, y: x + y, lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explain the results shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481"
     ]
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y**2, xs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481"
     ]
    }
   ],
   "source": [
    "np.sum(xs**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72439"
     ]
    }
   ],
   "source": [
    "x.fold(0, lambda x, y: x + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481"
     ]
    }
   ],
   "source": [
    "x.aggregate(0, lambda x, y: x + y**2, lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explain the results shown belwo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 5, 9], [5, 9, 9], [3, 5, 7], [4, 4, 3]]"
     ]
    }
   ],
   "source": [
    "x.fold([], lambda acc, val: acc + [val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 5, 9, 5, 9, 9, 3, 5, 7, 4, 4, 3]"
     ]
    }
   ],
   "source": [
    "seqOp = lambda acc, val: acc + [val]\n",
    "combOp = lambda acc, val: acc + val\n",
    "x.aggregate([], seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3,4])\n",
    "y = sc.parallelize([3,3,4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5]"
     ]
    }
   ],
   "source": [
    "x.map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]"
     ]
    }
   ],
   "source": [
    "x.filter(lambda x: x%3 == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Think of flatMap as a map followed by a flatten operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 0, 1, 1, 2, 2, 3]"
     ]
    }
   ],
   "source": [
    "x.flatMap(lambda x: range(x-2, x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]"
     ]
    }
   ],
   "source": [
    "x.sample(False, 0.5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-like transformss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 3]"
     ]
    }
   ],
   "source": [
    "y.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 3, 3, 4, 6]"
     ]
    }
   ],
   "source": [
    "x.union(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4]"
     ]
    }
   ],
   "source": [
    "x.intersection(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]"
     ]
    }
   ],
   "source": [
    "x.subtract(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (1, 3), (1, 4), (1, 6), (2, 3), (2, 3), (2, 4), (2, 6), (3, 3), (3, 3), (3, 4), (3, 6), (4, 3), (4, 3), (4, 4), (4, 6)]"
     ]
    }
   ],
   "source": [
    "x.cartesian(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that flatmap gets rid of empty lists, and is a good way to ignore \"missing\" or \"malformed\" entires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x):\n",
    "    try:\n",
    "        return [float(x)]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thee', 'square', 'root', 'of', '3', 'is', 'less', 'than', '3.14', 'unless', 'you', 'divide', 'by', '0']"
     ]
    }
   ],
   "source": [
    "s = \"Thee square root of 3 is less than 3.14 unless you divide by 0\".split()\n",
    "x = sc.parallelize(s)\n",
    "\n",
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [3.0], [], [], [], [3.14], [], [], [], [], [0.0]]"
     ]
    }
   ],
   "source": [
    "x.map(conv).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 3.14, 0.0]"
     ]
    }
   ],
   "source": [
    "x.flatMap(conv).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with key-value pairs\n",
    "----\n",
    "\n",
    "RDDs consissting of key-value pairs are required for many Spark operatinos. They can be created by using a function that returns an RDD composed of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('ann', 'spring', 'math', 98),\n",
    "        ('ann', 'fall', 'bio', 50),\n",
    "        ('bob', 'spring', 'stats', 100),\n",
    "        ('bob', 'fall', 'stats', 92),\n",
    "        ('bob', 'summer', 'stats', 100),\n",
    "        ('charles', 'spring', 'stats', 88),\n",
    "        ('charles', 'fall', 'bio', 100)   \n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ann', 'ann', 'bob', 'bob', 'bob', 'charles', 'charles']"
     ]
    }
   ],
   "source": [
    "rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ann', 'spring', 'math', 98), ('ann', 'fall', 'bio', 50), ('bob', 'spring', 'stats', 100), ('bob', 'fall', 'stats', 92), ('bob', 'summer', 'stats', 100), ('charles', 'spring', 'stats', 88), ('charles', 'fall', 'bio', 100)]"
     ]
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions `ByKey`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum values by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ann', 148), ('bob', 292), ('charles', 188)]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    rdd.\n",
    "    map(lambda x: (x[0], x[3])).\n",
    "    reduceByKey(lambda x, y: x + y).\n",
    "    collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running list of values by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ann', [50, 98]), ('bob', [100, 92, 100]), ('charles', [88, 100])]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    rdd.\n",
    "    map(lambda x: ((x[0], x[3]))).\n",
    "    aggregateByKey([], lambda x, y: x + [y], lambda x, y: x + y).\n",
    "    collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ann', 74), ('bob', 97), ('charles', 94)]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    rdd.\n",
    "    map(lambda x: ((x[0], x[3]))).\n",
    "    aggregateByKey([], lambda x, y: x + [y], lambda x, y: x + y).\n",
    "    map(lambda x: (x[0], sum(x[1])/len(x[1]))).\n",
    "    collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a different key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stats', 95), ('bio', 75), ('math', 98)]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    rdd.\n",
    "    map(lambda x: ((x[2], x[3]))).\n",
    "    aggregateByKey([], lambda x, y: x + [y], lambda x, y: x + y).\n",
    "    map(lambda x: (x[0], sum(x[1])/len(x[1]))).\n",
    "    collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using key-value pairs to find most frequent words in Ulysses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://rapid-777.oit.duke.edu:8020/data/texts/Portrait.txt\n",
      "hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt"
     ]
    }
   ],
   "source": [
    "hadoop = sc._jvm.org.apache.hadoop\n",
    "\n",
    "fs = hadoop.fs.FileSystem\n",
    "conf = hadoop.conf.Configuration() \n",
    "path = hadoop.fs.Path('/data/texts')\n",
    "\n",
    "for f in fs.get(conf).listStatus(path):\n",
    "    print f.getPath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulysses = sc.textFile('/data/texts/Portrait.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also read in entire docs as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'hdfs://rapid-777.oit.duke.edu:8020/data/texts/Ulysses.txt', u'hdfs://rapid-777.oit.duke.edu:8020/data/texts/Portrait.txt']"
     ]
    }
   ],
   "source": [
    "docs = sc.wholeTextFiles('/data/texts/')\n",
    "docs.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'', u\"Project Gutenberg's A Portrait of the Artist as a Young Man, by James\", u'Joyce', u'', u'This eBook is for the use of anyone anywhere at no cost and with almost', u'no restrictions whatsoever.  You may copy it, give it away or re-use it', u'under the terms of the Project Gutenberg License included with this', u'eBook or online at www.gutenberg.net', u'', u'']"
     ]
    }
   ],
   "source": [
    "ulysses.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def tokenize(line):\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "    return line.translate(table).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'project', u'gutenbergs', u'a', u'portrait', u'of', u'the', u'artist', u'as', u'a', u'young']"
     ]
    }
   ],
   "source": [
    "words = ulysses.flatMap(lambda line: tokenize(line))\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'project', 1), (u'gutenbergs', 1), (u'a', 1), (u'portrait', 1), (u'of', 1), (u'the', 1), (u'artist', 1), (u'as', 1), (u'a', 1), (u'young', 1)]"
     ]
    }
   ],
   "source": [
    "words = words.map(lambda x: (x, 1))\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'aided', 1), (u'softhued', 1), (u'vita', 1), (u'augustine', 2), (u'directions', 1), (u'hats', 3), (u'bear', 6), (u'crampton', 1), (u'yellow', 19), (u'four', 14)]"
     ]
    }
   ],
   "source": [
    "counts = words.reduceByKey(lambda x, y: x+y)\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 6052), (u'and', 3395), (u'of', 3259), (u'to', 2004), (u'a', 1986), (u'he', 1837), (u'his', 1743), (u'in', 1583), (u'was', 1066), (u'that', 951)]"
     ]
    }
   ],
   "source": [
    "counts.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count chained version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 6052), (u'and', 3395), (u'of', 3259), (u'to', 2004), (u'a', 1986), (u'he', 1837), (u'his', 1743), (u'in', 1583), (u'was', 1066), (u'that', 951)]"
     ]
    }
   ],
   "source": [
    "(\n",
    "ulysses.flatMap(lambda line: tokenize(line))\n",
    "                .map(lambda word: (word, 1))\n",
    "               .reduceByKey(lambda x, y: x + y)\n",
    "               .takeOrdered(10, key=lambda x: -x[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding slow Python UDF tokenize\n",
    "\n",
    "We will see how to to this in the DataFrames notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountByValue Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are sure that the results will fit into memory, you can get a dacitonary of counts more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = (\n",
    "    ulysses.\n",
    "    flatMap(lambda line: tokenize(line)).\n",
    "    countByValue()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6052"
     ]
    }
   ],
   "source": [
    "wc['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persisting data\n",
    "----\n",
    "\n",
    "The `top_word` program will repeat ALL the computations each time we take an action such as `takeOrdered`. We need to `persist` or `cache` the results - they are similar except that `persist` gives more control over how the data is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "counts.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[114] at RDD at PythonRDD.scala:48"
     ]
    }
   ],
   "source": [
    "counts.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "counts.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 6052), (u'and', 3395), (u'of', 3259), (u'to', 2004), (u'a', 1986)]"
     ]
    }
   ],
   "source": [
    "counts.takeOrdered(5, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'aided', 1), (u'universalis', 1), (u'softhued', 1), (u'vita', 1), (u'pardon', 1)]"
     ]
    }
   ],
   "source": [
    "counts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'1', 6), (u'10', 1), (u'11', 1), (u'13', 1), (u'14', 1)]"
     ]
    }
   ],
   "source": [
    "counts.takeOrdered(5, lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'aided', u'universalis', u'softhued', u'vita', u'pardon']"
     ]
    }
   ],
   "source": [
    "counts.keys().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]"
     ]
    }
   ],
   "source": [
    "counts.values().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "count_dict = counts.collectAsMap()\n",
    "count_dict['circle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using cache instead of persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[114] at RDD at PythonRDD.scala:48"
     ]
    }
   ],
   "source": [
    "counts.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "counts.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[114] at RDD at PythonRDD.scala:48"
     ]
    }
   ],
   "source": [
    "counts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "counts.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging key, value datasets\n",
    "\n",
    "We will build a second counts key: value RDD from another of Joyce's works - Portrait of the Artist as a Young Man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "portrait = sc.textFile('/data/texts/Portrait.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts1 = (\n",
    "portrait.flatMap(lambda line: tokenize(line))\n",
    "        .map(lambda x: (x, 1))\n",
    "        .reduceByKey(lambda x,y: x+y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[126] at RDD at PythonRDD.scala:48"
     ]
    }
   ],
   "source": [
    "counts1.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine counts for words found in both books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = counts.join(counts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'aided', (1, 1)), (u'softhued', (1, 1)), (u'augustine', (2, 2)), (u'hats', (3, 3)), (u'bear', (6, 6))]"
     ]
    }
   ],
   "source": [
    "joined.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sum counts over words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'aided', 2), (u'softhued', 2), (u'augustine', 4), (u'hats', 6), (u'bear', 12)]"
     ]
    }
   ],
   "source": [
    "s = joined.mapValues(lambda x: x[0] + x[1])\n",
    "s.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### average counts across books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'aided', 1.0), (u'softhued', 1.0), (u'augustine', 2.0), (u'hats', 3.0), (u'bear', 6.0)]"
     ]
    }
   ],
   "source": [
    "avg = joined.mapValues(lambda x: np.mean(x))\n",
    "avg.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
